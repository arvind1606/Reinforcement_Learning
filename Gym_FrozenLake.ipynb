{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will try to solve below problem using OpenAIGym environment.\n",
    "\n",
    "Frozen Lake:\n",
    "Winter is here, You and your friends are tossing around the frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "The water is mostly frozen, but there are a few holes where the ice has melted. If you stem in one of those holes, you will fall into the freezing water. T this time, there is an international frisbee shortage, so it's absolutely imperitive that navigate across the lake and retrive the disk. However the ice is slippry so you won't always move in the direction you intend. The surface is described using a grid like the following...  \n",
    "\n",
    "S F F F\n",
    "\n",
    "F H F H\n",
    "\n",
    "F F F H \n",
    "\n",
    "H F F G\n",
    "\n",
    "\n",
    "Here S represents the starting point, it is safe.\n",
    "F Frozon serface which is also safe\n",
    "H is Hole, if is not good\n",
    "G is Goal\n",
    "\n",
    "The agent can navigate Left, Right, Up and Down.\n",
    "It recieves the reward as 1 if it reches the goal, else 0.\n",
    "The episode ends when either the agent falls in Hole or reaches the Goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all lib\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember: Row count: size of state space in env\n",
    "#           Col count : size of action space in env\n",
    "# \n",
    "# This can be extracted as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space_size :  4\n",
      "state_space_size :  16\n"
     ]
    }
   ],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "print(\"action_space_size : \", action_space_size)\n",
    "print(\"state_space_size : \", state_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# will use above info to build the Q table\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets define all the required parameters\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1            # epsilone\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.001     # how big and how smalll our exploration rate can be\n",
    "exploration_decay_rate = 0.001   # Rate at which exploration rate will decayb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes = []\n",
    "\n",
    "# Q Learning Algo\n",
    "for episode in range(num_episodes):\n",
    "    # for each episode reset the env to initial state\n",
    "    state = env.reset()\n",
    "    \n",
    "    # done var will keep trackof if the episode is finished\n",
    "    done = False\n",
    "    # start with no rewards in the begining of episode\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        #exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        \n",
    "        # update Q table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "        \n",
    "    #Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "            (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rewards_all_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** avg reward per thousand episodes ******************\n",
      "\n",
      "1000  :  0.05300000000000004\n",
      "2000  :  0.21400000000000016\n",
      "3000  :  0.4090000000000003\n",
      "4000  :  0.5750000000000004\n",
      "5000  :  0.6870000000000005\n",
      "6000  :  0.7080000000000005\n",
      "7000  :  0.7260000000000005\n",
      "8000  :  0.7290000000000005\n",
      "9000  :  0.7380000000000005\n",
      "10000  :  0.7350000000000005\n",
      "***********updated Q table*************\n",
      "\n",
      "[[0.55873722 0.50358233 0.4983035  0.49801981]\n",
      " [0.3294781  0.34024087 0.28407139 0.52050647]\n",
      " [0.4107869  0.39516142 0.41334536 0.48090305]\n",
      " [0.32236689 0.277568   0.31970068 0.46311452]\n",
      " [0.57641444 0.3665535  0.22876746 0.36207069]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.16960956 0.22371334 0.3197786  0.17289626]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.44213139 0.4120752  0.37426615 0.61903489]\n",
      " [0.44455267 0.69104752 0.37867433 0.44014913]\n",
      " [0.57666129 0.42558925 0.41015798 0.32243559]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.53394351 0.44935107 0.78506269 0.46484768]\n",
      " [0.69610354 0.94228081 0.70837859 0.72442808]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Caclulate and print the avg reward per thousand episode\n",
    "reward_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "count = 1000\n",
    "print(\"************** avg reward per thousand episodes ******************\\n\")\n",
    "for r in reward_per_thousand_episodes:\n",
    "    print(count, \" : \" ,str(sum(r/1000)) )\n",
    "    count += 1000\n",
    "    \n",
    "    \n",
    "# print updated Q table\n",
    "print(\"***********updated Q table*************\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "You reached the Goal. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*********** EPISODE\", episode+1, \"**********\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        \n",
    "        action = np.argmax(q_table[state, :])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            \n",
    "            if reward == 1:\n",
    "                print(\"You reached the Goal. \\n\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"You fall throught a hole. \\n\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
